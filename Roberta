# -*- coding: utf-8 -*-
"""roberta.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CYKYQGaKzwEdO9-9JdTnpEGTJOrKtkjL
"""

pip install nbstripout
nbstripout --extra-keys "metadata.widgets metadata.widget" your_notebook.ipynb

pip install -q transformers datasets accelerate scikit-learn evaluate torch



"""from google.colab import files
uploaded = files.upload()
JSON_PATH = list(uploaded.keys())[0]
print("JSON_PATH =", JSON_PATH)
"""

import os, json, numpy as np, pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import f1_score, classification_report, confusion_matrix

import torch
from torch import nn

from datasets import Dataset
from transformers import (
    AutoTokenizer, AutoConfig,
    RobertaForSequenceClassification, Trainer, TrainingArguments
)
import evaluate

df=pd.read_json("filtered_complaints.json")
df.head()

# PARAMETRELER
TEXT_COL     = "complaint_what_happened"
TARGET_COL   = "product"
MODEL_NAME   = "roberta-base"   # gerekirse 'roberta-large'
MAX_LENGTH   = 512              # 384/512 dene
BATCH_SIZE   = 16               # GPU durumuna göre 8/16/32
LR           = 2e-5
EPOCHS       = 10
OUTPUT_DIR   = "/content/roberta_product_cls"
SEED         = 42
os.makedirs(OUTPUT_DIR, exist_ok=True)
np.random.seed(SEED); torch.manual_seed(SEED)

def read_any_json(path):
    try:
        # JSON array / dict -> pandas
        return pd.read_json(path)
    except ValueError:
        # JSON Lines
        return pd.read_json(path, lines=True)

df = read_any_json(JSON_PATH)

assert TEXT_COL in df.columns and TARGET_COL in df.columns, f"{TEXT_COL} ve {TARGET_COL} kolonları bulunamadı."
df = df[[TEXT_COL, TARGET_COL]].dropna()
df[TEXT_COL]   = df[TEXT_COL].astype(str).str.strip()
df[TARGET_COL] = df[TARGET_COL].astype(str).str.strip()

from collections import Counter
le = LabelEncoder()
df["label"] = le.fit_transform(df[TARGET_COL].values)
id2label = {i: lab for i, lab in enumerate(le.classes_)}
label2id = {v: k for k, v in id2label.items()}
num_labels = len(le.classes_)
print("Sınıf sayısı:", num_labels)
print("Sınıf dağılımı (ilk 10):", Counter(df["label"]).most_common(10))

with open(os.path.join(OUTPUT_DIR, "label_mapping.json"), "w", encoding="utf-8") as f:
    json.dump({"id2label": id2label, "label2id": label2id}, f, ensure_ascii=False, indent=2)

# Stratified split
train_df, test_df = train_test_split(
    df[[TEXT_COL, "label"]], test_size=0.2, random_state=SEED, stratify=df["label"]
)
train_df, val_df  = train_test_split(
    train_df, test_size=0.1, random_state=SEED, stratify=train_df["label"]
)

# Class weights (dengesizliğe karşı)
class_weights = compute_class_weight(
    class_weight="balanced",
    classes=np.arange(num_labels),
    y=train_df["label"].values
)
class_weights = torch.tensor(class_weights, dtype=torch.float)
print("Class weights:", class_weights.tolist())

# HF Datasets
train_ds = Dataset.from_pandas(train_df.reset_index(drop=True))
val_ds   = Dataset.from_pandas(val_df.reset_index(drop=True))
test_ds  = Dataset.from_pandas(test_df.reset_index(drop=True))

#  Tokenizer
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)

def tok_fn(batch):
    return tokenizer(
        batch[TEXT_COL],
        truncation=True,
        padding="max_length",
        max_length=MAX_LENGTH
    )

train_ds = train_ds.map(tok_fn, batched=True)
val_ds   = val_ds.map(tok_fn, batched=True)
test_ds  = test_ds.map(tok_fn, batched=True)

cols_to_keep = ["input_ids", "attention_mask", "label"]
train_ds.set_format(type="torch", columns=cols_to_keep)
val_ds.set_format(type="torch", columns=cols_to_keep)
test_ds.set_format(type="torch", columns=cols_to_keep)

# Weighted loss için custom Roberta
class WeightedRobertaForSequenceClassification(RobertaForSequenceClassification):
    def __init__(self, config, class_weights=None):
        super().__init__(config)
        self.class_weights = class_weights

    def compute_loss(self, model_outputs, labels):
        logits = model_outputs.logits
        loss_fct = nn.CrossEntropyLoss(weight=self.class_weights.to(logits.device))
        return loss_fct(logits.view(-1, self.num_labels), labels.view(-1))

config = AutoConfig.from_pretrained(
    MODEL_NAME,
    num_labels=num_labels,
    id2label=id2label,
    label2id=label2id
)
model = WeightedRobertaForSequenceClassification.from_pretrained(
    MODEL_NAME, config=config, class_weights=class_weights
)

acc_metric = evaluate.load("accuracy")
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    acc = acc_metric.compute(predictions=preds, references=labels)
    f1m = f1_score(labels, preds, average="macro")
    return {"accuracy": acc, "macro_f1": f1m}

!pip install -U transformers accelerate

# ==== RoBERTa: HF Dataset -> DataLoader (collate_fn) + Early Stopping (TEK HÜCRE) ====
import os, math, numpy as np, torch
from torch.utils.data import DataLoader
from transformers import AutoConfig, AutoModelForSequenceClassification, get_linear_schedule_with_warmup
from sklearn.metrics import accuracy_score, f1_score

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
SEED   = 42
torch.manual_seed(SEED); np.random.seed(SEED)

# 0) HF Dataset'leri torch formatına al (garanti)
cols = ["input_ids","attention_mask","label"]
for ds in [train_ds, val_ds, test_ds]:
    try:
        ds.set_format(type="torch", columns=cols)
    except Exception:
        pass  # bazı eski sürümlerde zaten torch döner

# 1) collate_fn: HF dict -> batch tensörleri
def hf_collate(batch):
    return {
        "input_ids": torch.stack([x["input_ids"] for x in batch]),
        "attention_mask": torch.stack([x["attention_mask"] for x in batch]),
        "labels": torch.tensor([int(x["label"]) for x in batch], dtype=torch.long),
    }

train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  collate_fn=hf_collate)
val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=hf_collate)
test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, collate_fn=hf_collate)

# 2) Model
# (num_labels tespiti)
try:
    num_labels = int(torch.max(train_ds["label"]).item()) + 1
except Exception:
    # yedek: train_df varsa
    num_labels = len(set(train_df["label"].tolist()))
config = AutoConfig.from_pretrained(MODEL_NAME, num_labels=num_labels)
model  = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=config).to(DEVICE)

# 3) Loss (varsa class_weights kullan)
try:
    loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights.to(DEVICE))
except Exception:
    loss_fn = torch.nn.CrossEntropyLoss()

# 4) Optimizasyon + Scheduler
optimizer   = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LR)
total_steps = len(train_loader) * EPOCHS
warmup_steps= int(0.1 * total_steps)
scheduler   = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)

# 5) AMP (fp16)
use_amp = torch.cuda.is_available()
scaler  = torch.cuda.amp.GradScaler(enabled=use_amp)

# 6) Train/Eval epoch koşucuları
def run_epoch(loader, train_mode=True):
    model.train() if train_mode else model.eval()
    losses, all_preds, all_labels = [], [], []

    for batch in loader:
        input_ids     = batch["input_ids"].to(DEVICE)
        attention_mask= batch["attention_mask"].to(DEVICE)
        labels        = batch["labels"].to(DEVICE)

        with torch.set_grad_enabled(train_mode):
            if use_amp:
                with torch.cuda.amp.autocast():
                    out   = model(input_ids=input_ids, attention_mask=attention_mask)
                    logits= out.logits
                    loss  = loss_fn(logits, labels)
            else:
                out   = model(input_ids=input_ids, attention_mask=attention_mask)
                logits= out.logits
                loss  = loss_fn(logits, labels)

            if train_mode:
                optimizer.zero_grad(set_to_none=True)
                if use_amp:
                    scaler.scale(loss).backward()
                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                    scaler.step(optimizer); scaler.update()
                else:
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                    optimizer.step()
                scheduler.step()

        losses.append(loss.item())
        preds = torch.argmax(logits, dim=1).detach().cpu().numpy()
        labs  = labels.detach().cpu().numpy()
        all_preds.extend(preds); all_labels.extend(labs)

    avg_loss = float(np.mean(losses))
    acc      = float(accuracy_score(all_labels, all_preds))
    f1m      = float(f1_score(all_labels, all_preds, average="macro"))
    return avg_loss, acc, f1m

# 7) Early Stopping (val_loss)
PATIENCE       = 3
best_val_loss  = float("inf")
epochs_no_impr = 0
best_ckpt_path = os.path.join(OUTPUT_DIR, "best_roberta_val_loss.pt")
os.makedirs(OUTPUT_DIR, exist_ok=True)

history = []
print(f"Training on {DEVICE} | epochs={EPOCHS} | patience={PATIENCE} | amp={use_amp}")

for ep in range(1, EPOCHS + 1):
    train_loss, train_acc, train_f1 = run_epoch(train_loader, train_mode=True)
    val_loss,   val_acc,   val_f1   = run_epoch(val_loader,   train_mode=False)

    print(f"Epoch {ep:02d}/{EPOCHS} | "
          f"train_loss: {train_loss:.4f}  train_acc: {train_acc:.4f}  "
          f"val_loss: {val_loss:.4f}  val_acc: {val_acc:.4f}  val_macroF1: {val_f1:.4f}")

    history.append({
        "epoch": ep,
        "train_loss": train_loss, "train_acc": train_acc, "train_macro_f1": train_f1,
        "val_loss": val_loss,     "val_acc": val_acc,     "val_macro_f1": val_f1
    })

    if val_loss < best_val_loss - 1e-6:
        best_val_loss  = val_loss
        epochs_no_impr = 0
        torch.save({"model_state": model.state_dict(), "config": model.config.to_dict()}, best_ckpt_path)
    else:
        epochs_no_impr += 1
        if epochs_no_impr >= PATIENCE:
            print(f">>> Early stopping (patience={PATIENCE}). Best val_loss={best_val_loss:.4f}")
            break

# 8) En iyi modeli yükle + Test
if os.path.exists(best_ckpt_path):
    ckpt = torch.load(best_ckpt_path, map_location=DEVICE)
    model.load_state_dict(ckpt["model_state"])
    print(f"Best checkpoint loaded: {best_ckpt_path}")

test_loss, test_acc, test_f1 = run_epoch(test_loader, train_mode=False)
print(f"TEST | loss: {test_loss:.4f}  acc: {test_acc:.4f}  macro_f1: {test_f1:.4f}")

# 9) History kaydet (opsiyonel)
import pandas as pd
hist_path = os.path.join(OUTPUT_DIR, "training_history.csv")
pd.DataFrame(history).to_csv(hist_path, index=False, encoding="utf-8")
print("History saved to:", hist_path)

from sklearn.metrics import classification_report

# --- Test loader'dan tüm tahminleri topla ---
all_preds, all_labels = [], []
model.eval()
with torch.no_grad():
    for batch in test_loader:
        input_ids      = batch["input_ids"].to(DEVICE)
        attention_mask = batch["attention_mask"].to(DEVICE)
        labels         = batch["labels"].to(DEVICE)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        preds   = torch.argmax(outputs.logits, dim=1)

        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

# --- Class isimlerini id2label'dan al ---
target_names = [id2label[i] for i in range(len(id2label))]

# --- Rapor ---
print("Classification Report (TEST):")
print(classification_report(all_labels, all_preds, target_names=target_names, digits=3))

import pandas as pd
import matplotlib.pyplot as plt


hist_df = pd.DataFrame(history)

#  Loss grafiği
plt.figure(figsize=(8,5))
plt.plot(hist_df["epoch"], hist_df["train_loss"], label="Train Loss", marker="o")
plt.plot(hist_df["epoch"], hist_df["val_loss"],   label="Val Loss", marker="o")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Train vs Val Loss")
plt.legend()
plt.grid(True)
plt.show()

#  Accuracy grafiği
plt.figure(figsize=(8,5))
plt.plot(hist_df["epoch"], hist_df["train_acc"], label="Train Acc", marker="o")
plt.plot(hist_df["epoch"], hist_df["val_acc"],   label="Val Acc", marker="o")
plt.xlabel("Epoch")
plt

import numpy as np, pandas as pd
from sklearn.model_selection import train_test_split

TEXT_COL   = "complaint_what_happened"
TARGET_COL = "product"
THRESH     = 1000


mask = df[TARGET_COL].notna() & df[TEXT_COL].notna()
df_use = df.loc[mask, [TEXT_COL, TARGET_COL]].copy()
df_use[TEXT_COL] = df_use[TEXT_COL].astype(str)
df_use[TARGET_COL] = df_use[TARGET_COL].astype(str)

X = df_use[TEXT_COL].to_numpy()
y = df_use[TARGET_COL].to_numpy()

X_tr, X_te, y_tr, y_te = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

train_counts_before = pd.Series(y_tr).value_counts().sort_values(ascending=False)
test_counts_before  = pd.Series(y_te).value_counts().sort_values(ascending=False)

allowed_classes = train_counts_before[train_counts_before >= THRESH].index.tolist()
removed_classes = train_counts_before[train_counts_before < THRESH]

print("=== TRAIN sınıf sayıları (ÖNCE) ===")
print(train_counts_before)
print("\n=== TEST sınıf sayıları (ÖNCE) ===")
print(test_counts_before)

print("\n=== Eşik altı (train<%d) sınıflar ===" % THRESH)
print(removed_classes if not removed_classes.empty else "Yok")

print("\nKalacak sınıflar (>= %d): %s" % (THRESH, list(allowed_classes)))

def filter_by_allowed(X_arr, y_arr, allowed):
    keep = np.isin(y_arr, allowed)
    return X_arr[keep], y_arr[keep]

X_tr_f, y_tr_f = filter_by_allowed(X_tr, y_tr, allowed_classes)
X_te_f, y_te_f = filter_by_allowed(X_te, y_te, allowed_classes)

train_counts_after = pd.Series(y_tr_f).value_counts().sort_values(ascending=False)
test_counts_after  = pd.Series(y_te_f).value_counts().sort_values(ascending=False)

print("\n=== TRAIN (SONRA, filtrelenmiş) ===")
print(train_counts_after)
print("Toplam train önce:", len(y_tr), "→ sonra:", len(y_tr_f),
      "| Sınıf sayısı önce:", len(train_counts_before), "→ sonra:", len(train_counts_after))

print("\n=== TEST (SONRA, filtrelenmiş) ===")
print(test_counts_after)
print("Toplam test önce:", len(y_te), "→ sonra:", len(y_te_f),
      "| Sınıf sayısı önce:", len(test_counts_before), "→ sonra:", len(test_counts_after))


X_tr, y_tr, X_te, y_te = X_tr_f, y_tr_f, X_te_f, y_te_f
print("Train (filtered):", len(y_tr), " | Test (filtered):", len(y_te))

# ==== RoBERTa: text / target kolonlarıyla eğitim ====
import os, math, numpy as np, pandas as pd, torch, matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import accuracy_score, f1_score, classification_report
from datasets import Dataset
from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification, get_linear_schedule_with_warmup
from torch.utils.data import DataLoader
from tqdm.auto import tqdm

# ---- Hiperparametreler ----
MODEL_NAME = "roberta-base"
MAX_LENGTH = 512
BATCH_SIZE = 16
LR         = 2e-5
EPOCHS     = 10
PATIENCE   = 3
SEED       = 42
OUTPUT_DIR = "/content/roberta_cls_text_target"
DEVICE     = "cuda" if torch.cuda.is_available() else "cpu"
os.makedirs(OUTPUT_DIR, exist_ok=True)
np.random.seed(SEED); torch.manual_seed(SEED)

# ---- 1) Train -> Val ayır ----
train_df, val_df = train_test_split(
    pd.DataFrame({"text": X_tr, "target": y_tr}),
    test_size=0.2, random_state=SEED, stratify=y_tr
)
test_df = pd.DataFrame({"text": X_te, "target": y_te})

def show_counts(title, yarr, top=10):
    vc = pd.Series(yarr).value_counts().sort_values(ascending=False)
    print(f"\n=== {title} ===")
    print("Toplam:", len(yarr), "| Sınıf sayısı:", len(vc))
    print(vc.head(top))
    return vc

show_counts("TRAIN sınıf sayıları", train_df["target"])
show_counts("VAL sınıf sayıları",   val_df["target"])
show_counts("TEST sınıf sayıları",  test_df["target"])

# ---- 2) Label encode ----
le = LabelEncoder()
train_df["label"] = le.fit_transform(train_df["target"])
val_df["label"]   = le.transform(val_df["target"])
test_df["label"]  = le.transform(test_df["target"])

id2label   = {i: c for i, c in enumerate(le.classes_)}
label2id   = {c: i for i, c in id2label.items()}
num_labels = len(le.classes_)
print("\nKalan sınıf sayısı:", num_labels)

# ---- 3) HF Dataset + Tokenizer ----
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)

def tok_fn(batch):
    return tokenizer(batch["text"], truncation=True, padding="max_length", max_length=MAX_LENGTH)

tr_ds  = Dataset.from_pandas(train_df[["text","label"]]).map(tok_fn, batched=True)
val_ds = Dataset.from_pandas(val_df[["text","label"]]).map(tok_fn, batched=True)
te_ds  = Dataset.from_pandas(test_df[["text","label"]]).map(tok_fn, batched=True)

cols = ["input_ids","attention_mask","label"]
for ds in [tr_ds, val_ds, te_ds]:
    ds.set_format(type="torch", columns=cols)

def hf_collate(batch):
    return {
        "input_ids": torch.stack([x["input_ids"] for x in batch]),
        "attention_mask": torch.stack([x["attention_mask"] for x in batch]),
        "labels": torch.tensor([int(x["label"]) for x in batch], dtype=torch.long),
    }

train_loader = DataLoader(tr_ds,  batch_size=BATCH_SIZE, shuffle=True,  collate_fn=hf_collate)
val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=hf_collate)
test_loader  = DataLoader(te_ds,  batch_size=BATCH_SIZE, shuffle=False, collate_fn=hf_collate)

# ---- 4) Model + class weights ----
class_weights_np = compute_class_weight(class_weight="balanced",
                                        classes=np.arange(num_labels),
                                        y=train_df["label"].values)
class_weights = torch.tensor(class_weights_np, dtype=torch.float, device=DEVICE)

config = AutoConfig.from_pretrained(MODEL_NAME, num_labels=num_labels, id2label=id2label, label2id=label2id)
model  = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=config).to(DEVICE)
loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)

# ---- 5) Optimizer + Scheduler + AMP ----
optimizer   = torch.optim.AdamW((p for p in model.parameters() if p.requires_grad), lr=LR)
total_steps = len(train_loader) * EPOCHS
warmup_steps= int(0.1 * total_steps)
scheduler   = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)
use_amp     = torch.cuda.is_available()
scaler      = torch.amp.GradScaler("cuda", enabled=use_amp)

def run_epoch(loader, train_mode=True):
    model.train() if train_mode else model.eval()
    losses, all_preds, all_labels = [], [], []
    pbar = tqdm(loader, disable=False)
    for batch in pbar:
        ids   = batch["input_ids"].to(DEVICE)
        mask  = batch["attention_mask"].to(DEVICE)
        labs  = batch["labels"].to(DEVICE)
        with torch.set_grad_enabled(train_mode):
            with torch.amp.autocast("cuda", enabled=use_amp):
                logits = model(input_ids=ids, attention_mask=mask).logits
                loss   = loss_fn(logits, labs)
            if train_mode:
                optimizer.zero_grad(set_to_none=True)
                scaler.scale(loss).backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                scaler.step(optimizer); scaler.update()
                scheduler.step()
        losses.append(loss.item())
        preds = torch.argmax(logits, dim=1).detach().cpu().numpy()
        labs_ = labs.detach().cpu().numpy()
        all_preds.extend(preds); all_labels.extend(labs_)
        pbar.set_description(f"{'Train' if train_mode else 'Val'} loss {np.mean(losses):.4f}")
    return float(np.mean(losses)), float(accuracy_score(all_labels, all_preds)), float(f1_score(all_labels, all_preds, average="macro"))

# ---- 6) Early Stopping ----
best_val_loss  = float("inf"); epochs_no_impr = 0
best_ckpt_path = os.path.join(OUTPUT_DIR, "best_roberta_val_loss.pt")
history = []

for ep in range(1, EPOCHS+1):
    print(f"\n===== EPOCH {ep}/{EPOCHS} =====")
    train_loss, train_acc, train_f1 = run_epoch(train_loader, True)
    val_loss,   val_acc,   val_f1   = run_epoch(val_loader,   False)
    print(f"Epoch {ep:02d} | train_loss: {train_loss:.4f}  train_acc: {train_acc:.4f}  "
          f"val_loss: {val_loss:.4f}  val_acc: {val_acc:.4f}  val_macroF1: {val_f1:.4f}")
    history.append({"epoch":ep,"train_loss":train_loss,"train_acc":train_acc,"train_macro_f1":train_f1,
                    "val_loss":val_loss,"val_acc":val_acc,"val_macro_f1":val_f1})
    if val_loss < best_val_loss - 1e-6:
        best_val_loss, epochs_no_impr = val_loss, 0
        torch.save({"model_state": model.state_dict(), "config": model.config.to_dict()}, best_ckpt_path)
    else:
        epochs_no_impr += 1
        if epochs_no_impr >= PATIENCE:
            print(f">>> Early stopping (patience={PATIENCE}). Best val_loss={best_val_loss:.4f}")
            break

# ---- 7) En iyi modeli yükle + TEST ----
if os.path.exists(best_ckpt_path):
    ckpt = torch.load(best_ckpt_path, map_location=DEVICE)
    model.load_state_dict(ckpt["model_state"])
    print(f"Best checkpoint loaded: {best_ckpt_path}")

def predict_all(loader):
    model.eval()
    all_preds, all_labels = [], []
    with torch.no_grad():
        for batch in loader:
            ids   = batch["input_ids"].to(DEVICE)
            mask  = batch["attention_mask"].to(DEVICE)
            labs  = batch["labels"].to(DEVICE)
            logits = model(input_ids=ids, attention_mask=mask).logits
            preds  = torch.argmax(logits, dim=1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labs.cpu().numpy())
    return np.array(all_labels), np.array(all_preds)

y_true, y_pred = predict_all(test_loader)
print(f"\nTEST | acc: {accuracy_score(y_true,y_pred):.4f}  macro_f1: {f1_score(y_true,y_pred,average='macro'):.4f}")

target_names = [id2label[i] for i in range(num_labels)]
print("\nClassification Report (TEST):")
print(classification_report(y_true, y_pred, target_names=target_names, digits=3, zero_division=0))

# ---- 8) Grafikler ----
hist_df = pd.DataFrame(history)
plt.figure(figsize=(7,4)); plt.plot(hist_df["epoch"], hist_df["train_loss"], marker="o", label="Train Loss")
plt.plot(hist_df["epoch"], hist_df["val_loss"],   marker="o", label="Val Loss")
plt.xlabel("Epoch"); plt.ylabel("Loss"); plt.title("Loss"); plt.grid(True); plt.legend(); plt.show()

plt.figure(figsize=(7,4)); plt.plot(hist_df["epoch"], hist_df["train_acc"], marker="o", label="Train Acc")
plt.plot(hist_df["epoch"], hist_df["val_acc"],   marker="o", label="Val Acc")
plt.xlabel("Epoch"); plt.ylabel("Accuracy"); plt.title("Accuracy"); plt.grid(True); plt.legend(); plt.show()













